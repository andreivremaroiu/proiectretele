{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predictive Modeling of Television Subscription Rates Using Regression Analysis\n",
        "## Students:\n",
        "* Vremăroiu Andrei Florin\n",
        "* Tudor Alexandru Panait\n",
        "\n",
        "## Objectives\n",
        "**Data Analysis:** The project will analyze historical data from television transmitters, stations, inflation rates, and subscription figures.\n",
        "\n",
        "**Model Development:** We aim to develop regression models to predict television subscription rates using variables like transmitter and station numbers, and inflation rates.\n",
        "\n",
        "**Model Evaluation:** The models' effectiveness will be assessed using metrics like Mean Squared Error (MSE) and Mean Absolute Error (MAE).\n",
        "\n",
        "**Hyperparameter Optimization:** This step will fine-tune the Random Forest Regressor's hyperparameters to enhance prediction accuracy.\n",
        "\n",
        "**Business Insights:** The project seeks to uncover insights into the factors affecting television subscription rates and their business implications in the telecom industry."
      ],
      "metadata": {
        "id": "FcOHgZnvf1PC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> *Please run all cells and upload the dataset from the folder. Thank you!*\n",
        "\n"
      ],
      "metadata": {
        "id": "jL0P2tr_jCuf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "uFgsKnVbf0LJ",
        "outputId": "7ca4f8e0-6ea8-4151-b5fb-d4f95f724607"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d4fd77c8-bc9e-4690-bfee-0f847c4077d6\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d4fd77c8-bc9e-4690-bfee-0f847c4077d6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving translatoareteleviziune.csv to translatoareteleviziune.csv\n",
            "Saving statiiteleviziune.csv to statiiteleviziune.csv\n",
            "Saving ratainflatiei.csv to ratainflatiei.csv\n",
            "Saving program.py to program.py\n",
            "Saving abonamenteteleviziune.csv to abonamenteteleviziune.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Necessary Libraries\n",
        "\n",
        "In this section, we import several Python libraries that are essential for data handling, model building, and performance evaluation:\n",
        "\n",
        "- **pandas**: Used for data manipulation and analysis. It provides data structures and operations for manipulating numerical tables and time series, which is fundamental for handling our datasets.\n",
        "\n",
        "- **scikit-learn (sklearn)**: This library is crucial for various stages of the machine learning pipeline:\n",
        "  - `train_test_split`: Helps in splitting the data into training and testing sets, which is necessary for training our models and evaluating their performance on unseen data.\n",
        "  - `LinearRegression`: Provides the implementation of the linear regression model, which we use as one of our predictive models to forecast television subscription rates.\n",
        "  - `mean_squared_error`, `mean_absolute_error`: These functions allow us to calculate the Mean Squared Error (MSE) and Mean Absolute Error (MAE) of our models, which are key metrics for evaluating the accuracy of our predictions.\n",
        "  - `RandomForestRegressor`: An ensemble learning method based on randomized decision trees, known for its high accuracy in regression tasks. We use it to build a more complex model that can potentially capture nonlinear dependencies in the data.\n",
        "  - `GridSearchCV`: A tool for tuning model parameters (hyperparameters) to find the most effective model settings. It automates the process of finding the best parameters for the models, enhancing their performance by optimizing hyperparameter settings.\n",
        "\n",
        "These libraries and their specific modules provide the tools needed to carry out each phase of our project, from data preparation to complex model training and evaluation.\n"
      ],
      "metadata": {
        "id": "GDWacyLxhI93"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDzhcKClf0LL"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WU5FCKmf0LM"
      },
      "outputs": [],
      "source": [
        "# Loading datasets\n",
        "translatoare_df = pd.read_csv('translatoareteleviziune.csv')\n",
        "statii_df = pd.read_csv('statiiteleviziune.csv')\n",
        "inflatie_df = pd.read_csv('ratainflatiei.csv')\n",
        "abonamente_df = pd.read_csv('abonamenteteleviziune.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "### Cleaning Column Names\n",
        "To ensure consistency and prevent errors during data manipulation, we first clean the column names across all datasets:\n",
        "- **Stripping whitespace**: We remove any leading or trailing spaces from the column names using `str.strip()` method. This is crucial to avoid errors in referencing column names that might inadvertently include spaces.\n",
        "- **Column selection**: For the `translatoare_df`, we only retain the first two columns as they contain the relevant data needed for our analysis.\n",
        "\n",
        "### Merging Datasets\n",
        "To consolidate our data for analysis, we merge the datasets on the Anul column, which represents the year:\n",
        "\n",
        "- **Merging strategy**: We use an inner join to ensure that we only keep records that have data across all datasets for the same year.\n",
        "- **Handling suffixes**: To differentiate columns with the same names but from different datasets, we add suffixes to the column names (_translatoare, _statii, _inflatie, _abonamente).\n",
        "\n",
        "### Sorting Data\n",
        "To maintain the temporal order, which is crucial for any time series analysis or any study where trends over time are relevant, we sort the data by the Anul column\n",
        "\n",
        "### Splitting Data into Training and Testing Sets\n",
        "To evaluate the performance of our predictive models, we split our dataset into training and testing sets:\n",
        "\n",
        "- **Training set**: Contains 80% of the data, used to train the models.\n",
        "- **Testing set**: Comprises the remaining 20%, used to test the model's predictive performance.\n",
        "- **Feature and target separation**: We separate the features (X) from the target variable (y), which in this case is Rata valoare, representing the subscription rate or value."
      ],
      "metadata": {
        "id": "6v4IAkpuhWCn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vupnxP7f0LM"
      },
      "outputs": [],
      "source": [
        "# Cleaning column names\n",
        "translatoare_df.columns = translatoare_df.columns.str.strip()\n",
        "translatoare_df = translatoare_df.iloc[:, :2]\n",
        "inflatie_df.columns = inflatie_df.columns.str.strip()\n",
        "abonamente_df.columns = abonamente_df.columns.str.strip()\n",
        "statii_df.columns = statii_df.columns.str.strip()\n",
        "\n",
        "# Merging datasets based on 'Anul' column\n",
        "merged_df = pd.merge(translatoare_df, statii_df, on='Anul', suffixes=('_translatoare', '_statii'))\n",
        "merged_df = pd.merge(merged_df, inflatie_df, on='Anul')\n",
        "merged_df = pd.merge(merged_df, abonamente_df, on='Anul', suffixes=('_inflatie', '_abonamente'))\n",
        "\n",
        "# Sorting DataFrame by 'Anul' column to maintain temporal order\n",
        "merged_df = merged_df.sort_values(by='Anul')\n",
        "\n",
        "# Splitting the data into training and testing sets\n",
        "train_index = int(0.8 * len(merged_df))\n",
        "X_train = merged_df.iloc[:train_index].drop('Rata valoare', axis=1)\n",
        "y_train = merged_df.iloc[:train_index]['Rata valoare']\n",
        "X_test = merged_df.iloc[train_index:].drop('Rata valoare', axis=1)\n",
        "y_test = merged_df.iloc[train_index:]['Rata valoare']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Development and Evaluation\n",
        "\n",
        "### Training the Linear Regression Model\n",
        "In this step, we employ the Linear Regression algorithm to develop our predictive model. This model will help us understand the relationship between the input features and the target variable, which in our case is the subscription rate.\n",
        "\n",
        "## Predicting and Evaluating the Model\n",
        "After training the model, we use it to predict subscription rates on the testing set. To assess the accuracy of our model's predictions, we calculate the Mean Squared Error (MSE) and Mean Absolute Error (MAE) between the predicted values and the actual values:\n",
        "\n",
        "- **Mean Squared Error (MSE)**: This metric measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. A lower MSE indicates a better fit of the model to the data.\n",
        "- **Mean Absolute Error (MAE)**: This metric measures the average magnitude of the errors in a set of predictions, without considering their direction. It's the average over the test sample of the absolute differences between prediction and actual observation where all individual differences are weighted equally.\n",
        "\n",
        "These evaluation metrics provide us with insights into the model's performance, indicating how well our model can forecast television subscription rates based on the given features. By examining these errors, we can gauge the accuracy and reliability of our predictive model."
      ],
      "metadata": {
        "id": "_HqNddPVh9Tl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YFgPZoSf0LN",
        "outputId": "3592e9c8-0b8a-40f9-baa2-35b375a5934d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.1336082006830817\n",
            "Mean Absolute Error: 0.27631422442122666\n"
          ]
        }
      ],
      "source": [
        "# Training a Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"Mean Absolute Error:\", mae)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Advanced Model Development and Evaluation\n",
        "\n",
        "### Training the Random Forest Regressor with Hyperparameter Optimization\n",
        "To enhance our predictive accuracy, we deploy the Random Forest Regressor, an ensemble learning method known for its robustness and higher accuracy in handling complex datasets with nonlinear relationships. To optimize the model, we employ GridSearchCV to systematically explore a range of hyperparameters, aiming to find the combination that yields the best prediction results.\n",
        "\n",
        "### Evaluating the Optimized Random Forest Model\n",
        "After determining the best hyperparameters, we use the optimized Random Forest model to make predictions on the test dataset. We then evaluate the model's accuracy using the same metrics as before: Mean Squared Error (MSE) and Mean Absolute Error (MAE). These metrics will provide a comparative insight into how the Random Forest model performs against the simpler Linear Regression model, highlighting improvements in prediction accuracy and model robustness.\n",
        "\n",
        "This section not only demonstrates the implementation of a more complex model but also emphasizes the importance of hyperparameter tuning in achieving the best possible outcomes from sophisticated machine learning algorithms."
      ],
      "metadata": {
        "id": "xjK9oca1iLgK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzAjsvAUf0LO",
        "outputId": "6056d7d6-90d6-448b-8020-c84d5d243883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 81 candidates, totalling 243 fits\n",
            "Best Hyperparameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
            "Mean Squared Error (Random Forest): 3.5191628509339474\n",
            "Mean Absolute Error (Random Forest): 1.7783228870870849\n"
          ]
        }
      ],
      "source": [
        "# Training a Random Forest Regressor model with hyperparameter optimization\n",
        "rf_model = RandomForestRegressor()\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "grid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "y_pred_rf = grid_search.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
        "print(\"Mean Squared Error (Random Forest):\", mse_rf)\n",
        "print(\"Mean Absolute Error (Random Forest):\", mae_rf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Boosting Model Training and Evaluation\n",
        "\n",
        "### Training the Gradient Boosting Model\n",
        "Gradient Boosting is a powerful and widely-used machine learning technique that builds on decision trees. Here, we train a Gradient Boosting Regressor which optimizes for least squares regression. The model parameters include:\n",
        "- `n_estimators=100`: The number of boosting stages to be run. More stages can lead to better performance but also to overfitting.\n",
        "- `max_depth=5`: The maximum depth of the individual regression estimators. This controls the complexity and performance of the model.\n",
        "\n",
        "### Evaluating the Gradient Boosting Model\n",
        "After training, we predict the television subscription rates using our test set and evaluate the model's performance using Mean Squared Error (MSE) and Mean Absolute Error (MAE) to measure accuracy.\n"
      ],
      "metadata": {
        "id": "n0do678Vm-Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training a Gradient Boosting model\n",
        "gb_model = GradientBoostingRegressor(n_estimators=100, max_depth=5)\n",
        "gb_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_model.predict(X_test)\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
        "\n",
        "# Output MSE and MAE for Gradient Boosting model\n",
        "print(\"Gradient Boosting Model Performance:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse_gb)\n",
        "print(\"Mean Absolute Error (MAE):\", mae_gb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "9bM4Garim2Gt",
        "outputId": "d5f65133-5843-4615-82e9-36b0cfb8ddff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'GradientBoostingRegressor' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f077dd43561f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training a Gradient Boosting model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgb_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred_gb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgb_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmse_gb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_gb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'GradientBoostingRegressor' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the XGBoost Model\n",
        "XGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library designed to be highly efficient, flexible, and portable. It implements machine learning algorithms under the Gradient Boosting framework. XGBoost provides a parallel tree boosting that solves many data science problems in a fast and accurate way. The key parameters are:\n",
        "\n",
        "- `objective=reg:squarederror`: Specifies the learning task and the corresponding learning objective.\n",
        "- `n_estimators=100`: Number of gradient boosted trees. Equivalent to the number of boosting rounds.\n",
        "- `learning_rate=0.1`: Boosting learning rate (xgb's \"eta\")\n",
        "- `max_depth=5`: Maximum depth of a tree. Increasing this value will make the model more complex and likely more likely to overfit.\n",
        "\n",
        "### Evaluating the XGBoost Model\n",
        "Similar to the Gradient Boosting model, we evaluate the XGBoost model's performance on the test data using MSE and MAE to understand its accuracy in predicting subscription rates."
      ],
      "metadata": {
        "id": "spFrkZ3pnHwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training an XGBoost model\n",
        "xgb_model = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
        "\n",
        "# Output MSE and MAE for XGBoost model\n",
        "print(\"XGBoost Model Performance:\")\n",
        "print(\"Mean Squared Error (MSE):\", mse_xgb)\n",
        "print(\"Mean Absolute Error (MAE):\", mae_xgb)"
      ],
      "metadata": {
        "id": "62OQFiaCm9Wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Performance Comparison and Visualization\n",
        "\n",
        "### Storing Performance Metrics\n",
        "In this section, we consolidate the performance metrics of all the models we have trained into a single DataFrame. This structure allows us to efficiently compare the Mean Squared Error (MSE) and Mean Absolute Error (MAE) across the following models:\n",
        "- Linear Regression\n",
        "- Random Forest\n",
        "- Gradient Boosting\n",
        "- XGBoost\n",
        "\n",
        "These metrics are crucial for evaluating the accuracy of each model, with MSE measuring the average of the squares of the errors (indicating the variance from the actual values), and MAE providing a linear score that represents the average magnitude of the errors.\n",
        "\n",
        "### Visualizing Model Comparison\n",
        "#### Mean Squared Error Comparison\n",
        "We create a bar chart to visually compare the MSE of each model. This graph highlights the model's performance in terms of error minimization, where a lower MSE value suggests a model with better predictive accuracy, indicating fewer and smaller errors in predictions.\n",
        "\n",
        "#### Mean Absolute Error Comparison\n",
        "Similarly, we plot the MAE for each model using a bar chart. This measure helps us understand which model predicts more closely to the actual values on average, with a lower MAE indicating a more accurate and consistent model.\n",
        "\n",
        "### Interpretation of Results\n",
        "These visualizations are instrumental in providing a clear and immediate comparison of model performance. By examining both MSE and MAE, we can determine not only which models perform best on average but also which are most reliable in terms of consistent prediction accuracy. This comprehensive analysis aids in making an informed decision about the best model to deploy for predicting television subscription rates based on the given dataset and project objectives."
      ],
      "metadata": {
        "id": "pIhk3FMPn-_R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing performance metrics in a DataFrame\n",
        "model_performance = pd.DataFrame({\n",
        "    \"Model\": [\"Linear Regression\", \"Random Forest\", \"Gradient Boosting\", \"XGBoost\"],\n",
        "    \"MSE\": [mse, mse_rf, mse_gb, mse_xgb],\n",
        "    \"MAE\": [mae, mae_rf, mae_gb, mae_xgb]\n",
        "})\n",
        "\n",
        "# Plotting MSE Comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(model_performance['Model'], model_performance['MSE'], color='blue')\n",
        "plt.title('Comparison of Models by MSE')\n",
        "plt.ylabel('Mean Squared Error')\n",
        "plt.show()\n",
        "\n",
        "# Plotting MAE Comparison\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(model_performance['Model'], model_performance['MAE'], color='green')\n",
        "plt.title('Comparison of Models by MAE')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Oefha_XXnrj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}